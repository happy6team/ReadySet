import os
from typing import Dict, Any, List, Optional, Callable, TypeVar, cast
from copy import deepcopy
from dotenv import load_dotenv
import openai
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.runnables.config import RunnableConfig
from agent_state import AgentState

import time

class ReportWritingGuideAgent:
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ í†µí•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì˜¨ í›„, ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•´ì£¼ëŠ” ì—ì´ì „íŠ¸
    """
    
    def __init__(
        self,
        db_path: str = "./vector_store/db",
        embedding_model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        openai_model: str = "gpt-4o-mini",
        temperature: float = 0.1,
        k: int = 5
    ):
        """
        ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œë¼ì¸ ì œê³µ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            db_path: ë²¡í„° DB ê²½ë¡œ
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            openai_model: OpenAI ëª¨ë¸ ì´ë¦„
            temperature: ìƒì„± ì˜¨ë„
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        """
        self.db_path = db_path
        self.embedding_model_name = embedding_model_name
        self.openai_model = openai_model
        self.temperature = temperature
        self.k = k
        
        # OpenAI API í‚¤ ë¡œë“œ
        load_dotenv()
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        openai.api_key = self.openai_api_key
        
        # ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ì´ˆê¸°í™”
        self._init_vector_db()
    
    def _init_vector_db(self) -> None:
        print("âœ… ReportWritingGuideAgent ì´ˆê¸°í™” ì™„ë£Œ")
        """ë²¡í„° DB ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            
            # ë²¡í„° DB ê²½ë¡œ í™•ì¸
            if not os.path.exists(self.db_path):
                raise FileNotFoundError(f"Vector DB not found at {self.db_path}")
            
            # ë²¡í„° DB ì´ˆê¸°í™”
            self.vectordb = Chroma(
                persist_directory=self.db_path,
                embedding_function=self.embeddings
            )
            print(f"Vector DB loaded from {self.db_path}")
            
        except Exception as e:
            self.vectordb = None
            self.embeddings = None
            print(f"Error initializing vector DB: {str(e)}")
            raise

    def generate_response(self, query: str, context: str) -> str:
        """
        OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            response = openai.chat.completions.create(
                model=self.openai_model,
                temperature=self.temperature,
                messages=[
                    {"role": "system", "content": "You are a report-writing expert. Based on the provided related documents, "
    "answer the user's query with a clear, structured guideline.\n"
    "Use **bold** for section titles and insert line breaks (`\\n`) between sections for readability.\n"
    "Return the result in **Korean** using markdown-friendly formatting."},
                    {"role": "user", "content": f"ì§ˆë¬¸: {query}\n\nê´€ë ¨ ë¬¸ì„œ:\n{context}"}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error generating response: {str(e)}")
            return f"ë‹µë³€ ìƒì„± ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def search_documents(self, query: str) -> Dict[str, Any]:
        if not self.vectordb:
            return {
                "answer": "ë²¡í„° DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "sources": [],
                "success": False
            }

        try:
            # âœ… ê²€ìƒ‰ ì‹œê°„ ì¸¡ì • ì‹œì‘
            search_start = time.perf_counter()
            retrieved_docs = self.vectordb.similarity_search(query=query, k=self.k)
            search_end = time.perf_counter()
            print(f"ğŸ” VectorDB ê²€ìƒ‰ ì‹œê°„: {search_end - search_start:.2f}ì´ˆ")

            context_entries = []
            sources = []

            for i, doc in enumerate(retrieved_docs):
                source_path = doc.metadata.get('source', 'Unknown')
                section = doc.metadata.get('section',
                                        doc.metadata.get('dl_meta', {}).get('headings', ['ë¯¸ë¶„ë¥˜ ì„¹ì…˜'])[0]
                                        if 'dl_meta' in doc.metadata else 'ë¯¸ë¶„ë¥˜ ì„¹ì…˜')
                filename = os.path.basename(source_path) if isinstance(source_path, str) else 'Unknown'

                context_entry = f"[ë¬¸ì„œ {i+1}] ì¶œì²˜: {filename}, ì„¹ì…˜: {section}\n{doc.page_content}"
                context_entries.append(context_entry)

                source_info = {
                    "content": doc.page_content,
                    "section": section,
                    "source": source_path,
                    "filename": filename,
                    "rank": i + 1
                }
                sources.append(source_info)

            context = "\n\n".join(context_entries)

            # âœ… LLM ì‘ë‹µ ìƒì„± ì‹œê°„ ì¸¡ì •
            gen_start = time.perf_counter()
            answer = self.generate_response(query, context)
            gen_end = time.perf_counter()
            print(f"ğŸ§  LLM ì‘ë‹µ ìƒì„± ì‹œê°„: {gen_end - gen_start:.2f}ì´ˆ")

            return {
                "answer": answer,
                "sources": sources,
                "context": context,
                "success": True
            }

        except Exception as e:
            print(f"Error searching documents: {str(e)}")
            return {
                "answer": f"ë¬¸ì„œ ê²€ìƒ‰ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "context": "",
                "success": False
            }

    def format_agent_response(self, search_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ì‘ë‹µ í˜•ì‹ êµ¬ì„±
        
        Args:
            search_result: ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            Dict: í˜•ì‹í™”ëœ ì‘ë‹µ
        """
        if search_result["success"]:
            message = {
                "answer": search_result["answer"],
                "sources": search_result["sources"][:3]  # ìƒìœ„ 3ê°œ ì†ŒìŠ¤ë§Œ í¬í•¨
            }
        else:
            message = {
                "error": search_result["answer"]
            }
            
        return {
            "messages": [message]
        }

    def invoke(self, state: AgentState, config: RunnableConfig) -> AgentState:
        """
        ì—ì´ì „íŠ¸ í˜¸ì¶œ ë©”ì„œë“œ
        
        Args:
            state: í˜„ì¬ ìƒíƒœ
            config: ì‹¤í–‰ ì„¤ì •
            
        Returns:
            AgentState: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
        """
        # ì…ë ¥ ì¿¼ë¦¬ ì¶”ì¶œ
        query = state.get("input_query", "")
        
        # ìŠ¤ë ˆë“œ ID ì¶”ì¶œ
        thread_id = (
            getattr(config, "configurable", {}).get("thread_id")
            if hasattr(config, "configurable")
            else config.get("thread_id", "default")
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        search_result = self.search_documents(query)
        
        # ì‘ë‹µ í˜•ì‹í™”
        response = self.format_agent_response(search_result)
        
        new_messages = list(state.get("messages", []))
        new_messages.extend(response["messages"])

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        return {
            **state,
            "messages": new_messages,
            "agent": "search_agent",
            "thread_id": thread_id,
            "raw_search_result": search_result  # ë””ë²„ê¹… ë° ê³ ê¸‰ ì‚¬ìš©ì„ ìœ„í•´ ì›ì‹œ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
        }


# ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í•¨ìˆ˜ í˜•íƒœë¡œ ë…¸ì¶œ
def create_search_agent(
    db_path: str = "./vector_store/db",
    embedding_model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    openai_model: str = "gpt-4o-mini"
) -> Callable[[AgentState, RunnableConfig], AgentState]:
    """
    ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œë¼ì¸ ì œê³µ ì—ì´ì „íŠ¸ ìƒì„± í•¨ìˆ˜
    
    Args:
        db_path: ë²¡í„° DB ê²½ë¡œ
        embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        openai_model: OpenAI ëª¨ë¸ ì´ë¦„
        
    Returns:
        Callable: ì—ì´ì „íŠ¸ í˜¸ì¶œ í•¨ìˆ˜
    """
    agent = ReportWritingGuideAgent(
        db_path=db_path,
        embedding_model_name=embedding_model_name,
        openai_model=openai_model
    )
    
    return agent

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤: ìµœì´ˆ 1íšŒë§Œ ìƒì„±ë¨
GLOBAL_AGENT = create_search_agent(
    db_path="./vector_store/db/reports_chroma",
    embedding_model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    openai_model="gpt-4o-mini"
)

# í¸ì˜ë¥¼ ìœ„í•œ í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
def invoke(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ ê²€ìƒ‰ ì—ì´ì „íŠ¸ í˜¸ì¶œ
    
    Args:
        state: í˜„ì¬ ìƒíƒœ
        config: ì‹¤í–‰ ì„¤ì •
        
    Returns:
        AgentState: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    
    # ì—ì´ì „íŠ¸ì˜ invoke ë©”ì„œë“œ í˜¸ì¶œ
    return GLOBAL_AGENT.invoke(state, config)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ìƒíƒœ ë° ì„¤ì •
    test_state = {"message": "íšŒì˜ë¡ ì‘ì„± ì‹œ ì˜ê²°ì‚¬í•­ì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ ì‘ì„±í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?"}
    test_config = RunnableConfig(configurable={"thread_id": "test-thread-123"})
    
    # ì—ì´ì „íŠ¸ í˜¸ì¶œ
    result = invoke(test_state, test_config)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ë³´ê³ ì„œ ê°€ì´ë“œë¼ì¸ ì œê³µ ì—ì´ì „íŠ¸ ì‘ë‹µ ===")
    messages = result.get("messages", [])
    if messages:
        if "answer" in messages[0]:
            print("\në‹µë³€:")
            print(messages[0]["answer"])
            
            print("\nì°¸ê³  ë¬¸ì„œ:")
            for i, source in enumerate(messages[0].get("sources", []), 1):
                print(f"\n[ì†ŒìŠ¤ {i}] - ì„¹ì…˜: {source.get('section', 'Unknown')}")
                print(f"ì¶œì²˜: {source.get('filename', 'Unknown')}")
                print(f"ë‚´ìš©: {source.get('content', '')[:150]}...")
        else:
            print("\nì˜¤ë¥˜:")
            print(messages[0].get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"))
    else:
        print("ì‘ë‹µ ì—†ìŒ")