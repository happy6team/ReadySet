import os
from typing import List, Dict
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

def initialize_employee_vectorstore():
    """ì§ì› ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ì„ êµ¬ì¡°í™”ëœ ë°©ì‹ìœ¼ë¡œ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤."""
    
    # 1. TXT íŒŒì¼ ë¡œë“œ
    txt_file_path = "vector_store/docs/employee_info"
    try:
        with open(txt_file_path, mode='r', encoding='utf-8') as file:
            text_content = file.read()
        print(f"âœ“ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(text_content)} ë°”ì´íŠ¸")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None
    
    # 2. í…ìŠ¤íŠ¸ë¥¼ í–‰ìœ¼ë¡œ
    lines = [line.strip() for line in text_content.split('\n') if line.strip()]
    
    # 3. ê° í–‰ì„ ì§ì› ì •ë³´ë¡œ íŒŒì‹±
    documents = []

    for line in lines:
        # ì‰¼í‘œë¡œ ë¶„ë¦¬ëœ ê°’ ì²˜ë¦¬
        parts = line.split(',')
        if len(parts) >= 5:  # ìµœì†Œí•œ 5ê°œ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            name = parts[0].strip()
            email = parts[1].strip()
            department = parts[2].strip()
            position = parts[3].strip()
            job_description = parts[4].strip()
            
            # ì§ì› ì •ë³´ í…ìŠ¤íŠ¸ ìƒì„±
            employee_info = f"""
            ì´ë¦„: {name}
            ì´ë©”ì¼: {email}
            ë¶€ì„œ: {department}
            ì§ì±…: {position}
            ë‹´ë‹¹ì—…ë¬´: {job_description}
            """
            # Document ê°ì²´ ìƒì„±
            doc = Document(page_content=employee_info)
            documents.append(doc)

    print(f"âœ“ {len(documents)}ëª…ì˜ ì§ì› ì •ë³´ íŒŒì‹± ì™„ë£Œ")

     # 4. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedding_model = OpenAIEmbeddings()
    
    # 5. ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ ì„¤ì •
    persist_path = f"vector_store/db/employee_info_chroma"

    # 6. ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(persist_path, exist_ok=True)

    # 7. Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=persist_path,
    )

    # 8. ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
    vectorstore.persist()
    print(f"ì§ì› ì •ë³´ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {persist_path}")
    
    return vectorstore

# VectorDB ë¡œë”©
def load_vectorstore():
    embedding = OpenAIEmbeddings()
    return Chroma(
        embedding_function=embedding,
        persist_directory="vector_store/db/employee_info_chroma"
    )

def match_person_for_query(query: str, project_name: str):
    vs = load_vectorstore()
    related_employees = vs.similarity_search(query, k=3)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # ì§ì› ì •ë³´ í¬ë§·íŒ…
    employee_info = ""
    for i, doc in enumerate(related_employees, 1):
        employee_info += f"ì§ì› {i}:\n{doc.page_content}\n\n"
    
    # ì§ì ‘ ì¶”ì²œ ìƒì„±
    prompt = f"""
You are the person-in-charge matching agent for {project_name}. Your role is to analyze the user's question and connect them with the appropriate person-in-charge.

1. Extract {project_name}-related task keywords from the user's question.  
2. Find and recommend the most relevant person-in-charge based on the extracted keywords.  
3. If the question is unrelated to {project_name} or no keywords can be extracted, respond with:  
   "No suitable person-in-charge found. Please ask a question related to {project_name}."

Response format:  
- If the question is related to {project_name}: Provide the person-in-charge information and the reason for the recommendation.  
- If the question is unrelated to {project_name}: Provide the message "No suitable person-in-charge found."

Employee information is as follows:  
{employee_info}

User question: {query}

Project: {project_name}

First, determine whether the question is related to {project_name}. If it is, select and recommend the single most appropriate person-in-charge from the above employee information.

IMPORTANT: Please make sure to generate in Korean language.
"""
    
    response = llm.invoke(prompt)
    return response.content

# LangGraph Supervisorìš© invoke í•¨ìˆ˜ 
def invoke(state: dict, config) -> dict:
    """LangGraph Supervisorìš© invoke í•¨ìˆ˜"""
    # ì…ë ¥ ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸°
    query = state.get("input_query", "")
    project_name = state.get("project_name", "ìŠ¤ë§ˆíŠ¸íŒœ í”„ë¡œì íŠ¸")  # ê¸°ë³¸ê°’ ì„¤ì •

    # ì„¤ì •ì—ì„œ thread_id ê°€ì ¸ì˜¤ê¸°
    thread_id = (
        getattr(config, "configurable", {}).get("thread_id")
        if hasattr(config, "configurable")
        else config.get("thread_id", "default")
    )

    # ë‹´ë‹¹ì ë§¤ì¹­ í•¨ìˆ˜ í˜¸ì¶œ
    result = match_person_for_query(query, project_name)
    # print(f"ë‹´ë‹¹ì ë§¤ì¹­ ê²°ê³¼:\n{result}")

    # messages ëˆ„ì 
    new_messages = list(state.get("messages", []))  # ê¸°ì¡´ ë©”ì‹œì§€ ìœ ì§€
    new_messages.append(f"ğŸ‘¨â€ğŸ’¼ ë‹´ë‹¹ì ë§¤ì¹­ ê²°ê³¼:\n{result}")

    return {
        **state,
        "messages": new_messages,
        "thread_id": thread_id,
        "matching_result": result  # ë§¤ì¹­ ê²°ê³¼ ì¶”ê°€
    }