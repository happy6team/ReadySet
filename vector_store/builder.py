import os
import argparse
import traceback
from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings



class VectorDatabaseBuilder:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        embedding_model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        chunk_size: int = 800,
        chunk_overlap: int = 150,
        db_path: str = "./vector_store/db"
    ):
        """
        VectorDatabaseBuilder ì´ˆê¸°í™”
        
        Args:
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
            db_path: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
        """
        self.embedding_model_name = embedding_model_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.db_path = db_path
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = self._init_embedding_model()
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )
    
    def _init_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            print(f"ì„ë² ë”© ëª¨ë¸ '{self.embedding_model_name}' ì´ˆê¸°í™” ì™„ë£Œ")
            return embeddings
        except Exception as e:
            print(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            raise
    
    def load_documents(self, file_paths: List[str]) -> List[Any]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œ íŒŒì¼ ë¡œë“œ
        
        Args:
            file_paths: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ ëª©ë¡
            
        Returns:
            ë¡œë“œëœ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        all_docs = []
        
        for file_path in file_paths:
            try:
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                
                loader = PyMuPDFLoader(file_path=file_path)
                docs = loader.load()
                all_docs.extend(docs)
                print(f"íŒŒì¼ '{os.path.basename(file_path)}' ë¡œë“œ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ ì¡°ê°")
            
            except Exception as e:
                print(f"íŒŒì¼ '{file_path}' ë¡œë“œ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                traceback.print_exc()
        
        print(f"ì´ ë¡œë“œëœ ë¬¸ì„œ ì¡°ê°: {len(all_docs)}ê°œ")
        return all_docs
    
    def process_documents(self, docs: List[Any]) -> tuple:
        """
        ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ìƒì„±
        
        Args:
            docs: ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            texts, metadatas: í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° íŠœí”Œ
        """
        all_chunks = []
        
        try:
            # ê° ë¬¸ì„œ ì²˜ë¦¬
            for i, doc in enumerate(docs):
                # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì—ì„œ ì„¹ì…˜ ì •ë³´ ì¶”ì¶œ
                source_path = doc.metadata.get("source", "")
                source_filename = os.path.basename(source_path) if source_path else "unknown"
                
                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
                metadata = {
                    "source": source_path,
                    "filename": source_filename,
                    "page": doc.metadata.get("page", 0),
                    "doc_index": i
                }
                
                # í…ìŠ¤íŠ¸ ë¶„í• 
                text = doc.page_content
                chunks = self.text_splitter.split_text(text)
                
                # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for j, chunk in enumerate(chunks):
                    all_chunks.append({
                        "text": chunk,
                        "metadata": {
                            **metadata,
                            "chunk_index": j,
                            "chunk_id": f"{i}_{j}"
                        }
                    })
            
            # ë²¡í„°í™”ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            texts = [chunk["text"] for chunk in all_chunks]
            metadatas = [chunk["metadata"] for chunk in all_chunks]
            
            print(f"ì´ {len(all_chunks)}ê°œì˜ ì²­í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            return texts, metadatas
        
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ë¶„í•  ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            raise
    
    def create_vector_db(self, texts: List[str], metadatas: List[Dict[str, Any]]) -> Optional[Chroma]:
        """
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            metadatas: ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ Chroma ê°ì²´ ë˜ëŠ” None (ì˜¤ë¥˜ ë°œìƒ ì‹œ)
        """
        try:
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            
            # ê¸°ì¡´ DB í™•ì¸
            if os.path.exists(self.db_path):
                print(f"ê²½ê³ : ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ê°€ '{self.db_path}'ì— ì¡´ì¬í•©ë‹ˆë‹¤. ë®ì–´ì“°ê¸°ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # ë²¡í„° DB ìƒì„±
            vectordb = Chroma.from_texts(
                texts=texts,
                embedding=self.embeddings,
                metadatas=metadatas,
                persist_directory=self.db_path
            )
            
            # ì €ì¥
            vectordb.persist()
            print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ '{self.db_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return vectordb
        
        except Exception as e:
            print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return None
    
    def build(self, file_paths: List[str]) -> Optional[Chroma]:
        """
        ì „ì²´ ë²¡í„° DB êµ¬ì¶• í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            file_paths: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ ëª©ë¡
            
        Returns:
            ìƒì„±ëœ Chroma ê°ì²´ ë˜ëŠ” None (ì˜¤ë¥˜ ë°œìƒ ì‹œ)
        """
        try:
            # 1. ë¬¸ì„œ ë¡œë“œ
            docs = self.load_documents(file_paths)
            if not docs:
                print("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # 2. ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í¬ ìƒì„±
            texts, metadatas = self.process_documents(docs)
            if not texts:
                print("ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # 3. ë²¡í„° DB ìƒì„± ë° ì €ì¥
            vectordb = self.create_vector_db(texts, metadatas)
            return vectordb
        
        except Exception as e:
            print(f"ë²¡í„° DB êµ¬ì¶• ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return None


def build_code_rule_vector_db(
    rule_file_path: str = "vector_store/docs/code_rules/coding_rules.txt",
    db_path: str = "vector_store/db/code_rule_chroma"
) -> Chroma:
    """
    ì½”ë“œ ì»¨ë²¤ì…˜ ê·œì¹™ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë²¡í„° DB ìƒì„±

    Args:
        rule_file_path: ì½”ë“œ ê·œì¹™ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        db_path: ì €ì¥ ê²½ë¡œ

    Returns:
        Chroma ê°ì²´
    """
    if not os.path.exists(rule_file_path):
        raise FileNotFoundError(f"ì½”ë“œ ê·œì¹™ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {rule_file_path}")

    with open(rule_file_path, "r", encoding="utf-8") as f:
        raw_text = f.read()

    chunks = [chunk.strip() for chunk in raw_text.split("\n\n") if chunk.strip()]
    documents = [Document(page_content=chunk) for chunk in chunks]

    embedding = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embedding,
        persist_directory=db_path
    )
    vectorstore.persist()
    print(f"âœ… ì½”ë“œ ê·œì¹™ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {db_path}")
    return vectorstore


def ensure_code_rule_vector_db_exists():
    """
    ì½”ë“œ ê·œì¹™ìš© ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ìƒì„±
    """
    db_path = "vector_store/db/code_rule_chroma"
    rule_file_path = "vector_store/docs/code_rules/coding_rules.txt"

    if os.path.exists(db_path) and os.path.isdir(db_path) and len(os.listdir(db_path)) > 0:
        print(f"âœ… ì½”ë“œ ê·œì¹™ ë²¡í„° DB ì´ë¯¸ ì¡´ì¬: {db_path}")
        return

    print("ğŸ“¦ ì½”ë“œ ê·œì¹™ ë²¡í„° DB ìƒì„± ì‹œì‘")
    build_code_rule_vector_db(rule_file_path=rule_file_path, db_path=db_path)

def ensure_vector_db_exists(db_path: str = "./vector_store/db", file_path: str="./docs"):
    # DBê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if os.path.exists(db_path) and os.path.isdir(db_path) and len(os.listdir(db_path)) > 0:
        print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ '{db_path}'ì— ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        return True
    
    print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤...")

    # ë²¡í„° DB ë¹Œë” ìƒì„±
    builder = VectorDatabaseBuilder(
        embedding_model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        chunk_size=400,
        chunk_overlap=50,
        db_path=db_path
    )
    
    # íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(file_path, str):
        if os.path.isdir(file_path):
            # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ëª¨ë“  íŒŒì¼ ê²½ë¡œë¥¼ ìˆ˜ì§‘
            file_paths = [os.path.join(file_path, f) for f in os.listdir(file_path) 
                         if os.path.isfile(os.path.join(file_path, f))]
        else:
            # ë‹¨ì¼ íŒŒì¼ì¸ ê²½ìš°
            file_paths = [file_path]
    else:
        file_paths = file_path
    
    # ë²¡í„° DB êµ¬ì¶•
    return builder.build(file_paths)
